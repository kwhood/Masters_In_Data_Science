# Masters_In_Data_Science

Project Portfolio 
Master’s in science in Applied Data Science

I.	Introduction

The ever-evolving field and study of data science is driving more business decisions and research than ever before, and letting data tell the stories that need to be heard. Data science as a field of study results in the ability to combine techniques such as data collection, analysis of statistical nature, data mining, visualization, and communication to a broad range of fields. It also incorporates the capability to use one’s intuitive nature and creativity applied to the extraction of insights from data. 

Throughout my study at Syracuse University in the Applied Data Science program, each course built upon the fundamentals that are required to succeed as a Data Scientist. As with any science field, we must first look at data and research, extract key insights, and apply an objective analysis, before the formulation of recommendations can commence. This requires critical thinking and the understanding of the business, the business problem, and how the information will be used. It is a strong trait that one must possess to be successful as a data scientist in any field, whether that be marketing, biomedical informatics, supply chain, or finance. 

With the ability to think critically about a business problem, tools are used to support the process of analyzing and extrapolating insights. Software such as Python, R, Jupiter Notebooks, Tensorflow, and others are used to explore data sets. These are used to not only pull in data using different methods such as APIs, csv files, txt files, xls files, but also to explore the data. The initial exploration of the data sets gives meaning to the overall distribution of the variables that lie within the data sets. The distribution of variables and visualizations that are created during data exploration, indicate the different methods that are used to analyze the data for insights. 

Different methods of extrapolating and analyzing are determined and then models are created, tuned, and fitted for the specific types of variables and outcomes. During experiments, models may overfit and rely too heavily on the training data and when predictions are done on the testing data the accuracy is low. Making adjustments to the algorithms within the model allow to correct any overfitting or underfitting that may occur. Some of the algorithms and models that were used in the projects are decision trees, Naïve Bayes, Keras, neural networks, and support vector machines just to name a few. 

While these models are helpful for predictions it is also imperative to pair these with the statistical methods of data science. Performing regressions and correlations, as well as ANOVAs, helps to understand what variables might have a negative or positive relationship. However, it is important to note that a correlation does not result in causation. These relationships that appear help to understand the population as whole from samples that were obtained. We are able to know approximately where the population means lie, as well as the confidence levels, and whether our models are statistically significant. Many of these techniques were used within the projects that are discussed below to determine the business impacts, as well key insights to creating better business decisions. The projects below also demonstrate and explain the learning objectives that were mastered within the Data Science program at Syracuse University. 

II.	Projects

A.	Let’s Wine About It

Reflection
Let's Wine About It was created by myself and a colleague during the Data Mining course at the iSchool at Syracuse University. This project allowed for focus on the statistical programming language of R, as well understanding different statistical models that were trained for prediction. The use of R helped to gain a better understanding for fitting our models, as well the learning of the language. First, we were able to download and pull in the data fairly easy using R. Then, we had to find the right mix of attributes within the model because many of the attributes had different statistical significance. Due to these differences it allowed us to narrow down the attributes that were consistent with good wine reviews. R allowed us to visualize and recognize patterns within the data. We did this by using ggplot to see the various distributions. Ggplot allowed us to have more control over what we wanted to visualize and determine which plots were best for the stakeholders to interpret. The patterns became more visible as we started to work with the data. We were able to notice patterns within the ratings given by different wine aficionados with some of the same chemical components. R also helped us to learn different ways of slicing data so that it was easier to work with when creating our prediction models. This project enabled the team to master the following: different statistical methods, data mining for patterns and prediction models, as well as visualization of the data. 

		Project Overview
The following project exemplifies the passion for the data life cycle and using classification modeling based on the chemical attributes of wine as well as the sensory rating determined by experts. It was very interesting to combine scientific attributes and objective ones to produce classification models. As we approached this project, we wanted to first classify the wine based on scientific attributes, then on variety, and finally how the wine experts rated the wine. We thought that if an expert rated the wine high, they might have a similar chemical make-up. 

The data was collected from the University of California Irvine repository and downloaded as csv files and loaded into R Studio. Various classification techniques were used, such as K Nearest Neighbor, Support Vector Machines, Random Forest, and Decision Trees. Throughout the test process we were testing for accuracy within these prediction classification models that were created. Some models took little to no to time to run, while others took a bit more, such as the Support Vector Machines. 

As lovers of good wines, it was important to us to gain the highest accuracy based on the chemical attributes and expert’s quality. We wanted to be able to find wines that had a good expert quality rating and chemical make-up. 

As a result, we thought that it would be nice to eventually create an app based on this method to use while shopping in the store for wine. It might help others to make better decisions when buying wines for events, celebrations, or just every day. 

Website Link: https://www.kirbywhood.com/let-s-wine-about-it
Google Drive Link: https://drive.google.com/drive/folders/13v7_53b9CFAoHXuckB227ZPF-WoQh_J6?usp=sharing

B.	Marketing That “Drives” Sales

Reflection
This project allowed us to use real world data that had the potential to influence a business’ marketing strategy. The team wanted to use this data because of the impact that it could have on a dealerships marketing budget. It gave us a chance to work directly with a client and create a strategy to determine what marketing channels they should use for advertisements. This came as a challenge because all of the data had to be collected and complied by hand and entered into excel. We had to work personally with the dealership to achieve this and make sure that we were staying on schedule with the timeline that we had created. As we dove into the data that we were able to collect, we realized that we were missing some data for analysis. The data that was missing would have accounted for how other dealers were doing in the market. We only knew where the brand resonated with the eight counties that surrounded the dealership. Due to this we had to change our focus a bit, and we did this by creating regression models in excel. These models showed us what variables that we needed to focus on. As the scope changed and the results started to appear it was more evident to us that we found better insights that we had originally planned. The team was able to create a comprehensive presentation that was not only presented to class but also to the dealership. Within this presentation, we were able to effectively communicate our findings and the implications that it had on their business. The dealership was beyond pleased. We even included recommendations that should be implemented to back the findings in the analysis. Being able to work with a client helped to master the automotive wording as well as understand why they place certain marketing buys. It gave us the ability to effectively show a business strategy that would drive new and used vehicle sales based on the marketing tactics that they chose. This was by far one of the biggest accomplishments within this program. The ability to produce an analysis and impact a business in a positive way was beyond satisfying. It helped the dealership grow in ways that were originally holding them back. This project enabled the team to have mastery of creating a strategy and then having to adapt that strategy based on the data, as well as, effectively communicate our findings, and develop a plan of action to implement the business decisions derived from the analysis. 

	Project Overview
Working in the automotive industry as a Data Analyst, it is important to understand what marketing tactics are driving new and used unit sales. This information is also vital to the dealership so that they are able to place the advertising buys accordingly and that will be effective. The following case study was particularly interesting in its findings. This project allowed for the mastery of understanding and implementing recommendations for an actual business based on findings. It also helped with the mastery of communication to a client about the analysis, tools, that were used, and helped them to understand the findings. The team collected the data by hand, which is not always the best method, but was the method that had to be done to collect the three years’ work

The research team collected the data by working closely with the dealership. They were able to collect three years’ worth of data for this analysis. Once all the data was retrieved, it was cleaned and organized in excel. Monthly sales and marketing expenditure were evaluated in order to predict profits for the dealership through optimizing their marketing opportunities. The original goal of the analysis was to create prediction models that would evaluate the current marketing tactics and optimize the spend for advertising. By the end of the analysis our scope had changed, and the results were intuitive, once they were uncovered, but gave great insight to what advertising was driving new and used vehicle sales. 

This project used excel and XLSTAT, regression models, and seemingly unrelated regressions using R, as well as controlled our models for competition within the market. 
 
The data set contained monthly average profit information for both new and used vehicles. Variables were chosen for the creation of two separate linear models to show what marketing channels were driving new and used vehicle sales, as well as determining the elasticities for each. In order to find the right mix of independent variables, several linear models were tested using features ranging from dollars spent on broadcasting to usage for the dealership's website. The subtraction method was then used and the variables that were not statistically significant were removed. Log transformations were applied to the dependent variables (average monthly profit * units sold). These transformations were applied to create a more defined linear relationship with profit, smoothing of extreme values and considering the elasticity of these variables. The elasticity was important to the models to explain the variable's sensitivity to a change in another variable. 

The first regression model was created to see the impact of the digital and broadcast marketing spend had on the sales of new vehicle sales. This model was imperative in indicating what marketing tactic was driving the new vehicle sales. The second model focused on what tactic was driving used vehicle sales. 

Within both models the adjusted R square explained how much of the variability in profit can be attributed to the two marketing channels, broadcast and digital. The t-stat and p-value (lower than an alpha of 0.05), showed which variable was most significant in driving the sales of new and used vehicles. The ANOVA p-value (Significance F) showed that our models were both significant at a 95% confidence level. Scatterplots were also used to see the relationship between the independent and dependent variables. Considering our sample size of data, and it being relatively small, the r-squared was also relatively small. 

The regression model for new vehicle sales showed an elasticity of 57% and used vehicles had an elasticity of 99%. This was a major finding for the team. If the dealership was not to do any advertising their used vehicle sales would drop significantly and would be impacted greatly. New vehicle sales would decrease by a little more than half if the dealership did not do any advertising. This is important for any dealer to understand the impact of their marketing spend has on their sales. 

Both models' results were then validated using seemingly unrelated regression, increasing the significance of each variable. Due to the size of the data collected, predictions were not able to be made as originally planned. However, the models that the team created offered valuable insight to new marketing strategies involving the investment of advertising spend to drive particular vehicle sales. The data was able to tell us which marketing tactics drove certain vehicle sales and how the placement of advertisements need to be where the dealership's customers are watching and/or leaving their digital footprint. 

Intuitively, we know that marketing results in brand recall and that results in sales. The ultimate results in this analysis was that digital marketing had an incremental effect on sales overall and was the primary driver of new vehicle sales. For every $1 increase in digital advertising spend it would return $0.11 in profit with 57% elasticity. Broadcast marketing tactics had an instrumental effect on used vehicle sales and for every $1 increase in broadcasting spend would return $0.70 in profit with 99.92% elasticity. The recommendations included continuing new vehicle OEM offers, rebates, leases and promotions on digital platforms to continue to drive new vehicle sales, as well as allocate broadcast spend to marketing used vehicles. These recommendations were based on the results of the models and these models that were created can be applied to any industry to evaluate their marketing spend across different tactics. 

Website Link: https://www.kirbywhood.com/marketing-that-drives-sales
Google Drive Link: https://drive.google.com/drive/folders/1eAmBX3RoHh1MFdc7jLGLl485Zza8gzNX?usp=sharing

C.	“The Individual” Perspective of Rights

Reflection
This was a paper that was completed as a requirement for the Information Policy course. At the time of taking this course I was the only data science focused individual, whereas the other students were focused on library science. This course was a relief from coding that challenged me to think about different points of view as well as to have an appreciation for others and their opinions. However, this course did have its challenges. At the beginning of the course it was very apparent that I did not think like the rest of my colleagues and I struggled with only thinking within the four walls of a library. I pushed myself to grasp the concepts of writing and understanding policies as well as the ethics that surround them. I spent many hours researching different angles of perspectives to gain an unbiased opinion. Knowing how data is being used in many ways for the greater good of humanity and solving the world’s problems, I found that there is not one universal policy to protect the individual’s rights to privacy within the vast world of the internet. Throughout the preparation for this paper, I chose to not only discuss one country as directed by the professor but gain a better understanding of how different parts of the world are handling the protection and ethical use of one’s personal information online. I feel that knowing how different countries treat an individual’s data that the United States will eventually adopt one of the policies in place, whether that be the GDPR or the Personal information Protection and Electronic Documents Act, both of which protect individual’s personal information. This paper helped to me to understand that there will be changes within the United States soon, as more policies surrounding the sharing of personal information across the world wide web start to be introduced and implemented. 

This paper also allowed me to expand my knowledge of policies within the ethical scope of data science. It did this by giving me the confidence to question the data that is being collected as well as the use of the data for analysis. Understanding how policies play a major role within data science helps to understand that the information is priceless and needs to be protected. Ethics lies at the heart of data science and it is up to individuals and businesses to protect that data from any breach that could occur. The learning objective that was mastered within this paper and class was the ability to synthesize the ethical dimensions of data science practice where privacy is concerned. 

Project Overview
Information being collected by companies worldwide remains a concern, however, most people are not aware of the actual size of their digital footprint. The following paper allowed for a deep dive into different countries and the policies that surround an individual's personal information and their rights. Every time that we access the internet, ask Alexa questions, or use our smartphones, we are leaving behind a digital trail that details how we live our daily lives. Have you ever gotten into your car to go somewhere, maybe home from work, and your phone will tell you the amount of time it will take for you to reach your destination? This is a common occurrence, and that information is now being sold and shared with businesses and other people around the globe.

Personal information in the digital age is still something that everyone should remain highly aware of who, or what has their information. Policies are evolving and taking into consideration the understanding that people are purchasing goods from many different countries, and how those businesses handle personal information across borders. Different societies, whether they are democratic or totalitarian view the rights of an individual differently and this has major implications on an individual's rights. 

Throughout the research that was done, I feel that the GDPR is currently the closest policy that takes into consideration the individual as a person and demands consent from that individual when it comes to the sharing of personal information. This policy is one that is of democratic views but is one that will hopefully be adopted by more countries worldwide. Similar to the GDPR, the Personal information Protection and Electronic Documents Act was also created with the intent to protect an individual's information from being shared or bought without their knowledge, and the individual must consent to the information that is being collected. 

It is understood that in countries that have a command economy and communist views, data drives industry and advances the agenda of the central committee and therefore all data is property of the government. Countries like this continuously monitor their population, going as far as to put sensors in sidewalks, and cameras on every corner. They wish to know every move of every citizen and considers it a way of policing their populations. 

Having experience in the marketing field, I understand that businesses see an individual's buying habits, credit score, health, and search patterns as a commodity and are highly sought after. This information can be used to serve targeted advertisements directly to an individual's device. However, it is an area that is beginning to become more regulated and protected. It is very interesting to me that many individuals continue to not consider their own privacy when interacting with the world wide web and give private parties and their government all their information, but yet will complain when they find out that these entities have their information. 

These are the same people that become outraged when governments are monitoring emails, chats, and web activity. When in reality governments are monitoring the internet for performance. Some governments will collect sensitive information about its citizens to help track crimes, but sensitive information is also being sold like a Big Mac at the McDonald's drive thru window. “For human rights to have any meaning, it is therefore essential for someone to be responsible for that data are used” (Buttarelli 2). It is ethical to think that someone above the individual should be responsible for monitoring the use of an individual's information, but in reality, it should start with the individual. A person is able to take different approaches to ensure that their sensitive information is protected, such as using private browsers, checking the fine print when signing up for something, and being cautious. 

The selling and use of individual’s information gives a new perspective on the world in which we live. Canada and the EU are setting precedence for the rest of the world by allowing the individual to regain control of their rights and their privacy. This allows the individual to not be scrutinized for their information and the ability to exercise their democratic right to freedom of expression. Taking away the democratic rights of the individual is most concerning. The US should turn to Canada and the EU for guidance to advance the technological policies for the better well-being of the individual. With their guidance the US could model policies after the GDPR, the ultimate setting of precedence in today’s society. Understanding the policies that surround such sensitive information allows a data scientist or someone who is collecting the data of an individual to know where the line is and how to properly obtain this information and protect it once it has been retrieved. 

Website Link: https://www.kirbywhood.com/survellance
Google Drive Link: https://drive.google.com/drive/folders/1va0WHRXuJacAHytrVs2Szl4Mw5VTjACE?usp=sharing 

D.	Image Classification: Protein Atlas

		Reflection
The image classification project was done for the Big Data class. This was one of the hardest and more advanced projects throughout the program. This project had its challenges, but as a team we were able to work through it. As a team we knew that we wanted to do an image classification project, and I had found the Protein Atlas competition on Kaggle. It took some convincing to use this data for our project, but we decided to take on this project and challenge our abilities. 

When we collected our data from the Kaggle competition site, we realized that we were going to need to come up with a strategy to be able to process the images. Having very strong computing systems, we needed to figure out how to approach images of this size and how to be efficient in our modeling due to time constraints. We did this by talking several times about ideas with the professor, whether that was building a new machine to handle this type of process or using Colab. We finally decided to use Colab because it allowed us to import our Jupyter notebooks and gave us enough GPU and ram to process these images. As we pulled in our data and started to process it, we ran into another snag, where we had to go back to the drawing board to decide what images and variables we were going to focus on for classification. We did this by looking at the impact each variable had on the human body as well as creating correlations and regressions. We finally narrowed down our focus more but had to sacrifice accuracy of our predictions for efficiency. As a team we made these decisions, because the models that were created took a lot of time to process and predict. By the end of the project we knew what we wanted to do if the experiment was conducted again. With image classification being very popular, I would like to continue to work on this project after completing my masters, as I feel it would be beneficial to tuning the models more and experimenting with different processing systems large enough for these images. This project shows the mastery of the learning objective to develop alternative strategies based on the data. 

		Project Overview		
Image classification is becoming very popular within many different fields including the medical industry. The team decided to look at many different human proteins to determine if there was an indication of disease. 

The data was obtained through the Human Protein Atlas Image Classification competition held on Kaggle. It consisted of over 30,000 images (512px by 512px), with 28 different labeled cell components, stored four times each with differently colored filters. Three of the filters specifically identified landmarks within the cells such as the nucleolus, golgi apparatus, endosomes, and lysosomes. The other was used for categorization. There was also the option to use the original 2048px by 2048px and 3072px by 3072px TIFF files instead of the scaled down PNGs, but that came out to around 250 GB and we were already anticipating struggling with the size of the images in CoLab with smaller files. 

The team ran a correlation on the different cell components to see if there was an indication that one component would be prominent with another if a disease was present. It made sense that certain labels would be pictured together more frequently than others based on their location in the cell. For instance, the most correlated duo were endosomes and lysosomes, whose life cycles actually include fusing with one another. 

Within each image, it was common to have 2 to 4 proteins that were prominent. Very few cells had 8 to 10 proteins. Knowing this we wanted to visualize a frequency count of the cell components that were more likely seen together. The nucleoplasm, cytosol, plasma membrane, and nucleoli were the cell components that appeared most often. 

We used the Kaggle API to bring in the data and narrowed down to only the images that were used for classification. We then unzipped the files, names, and class labels. The team chose to then narrow the focus and consider proteins that were most indicative of disease. The first protein chosen was the plasma membrane. This is the membrane surrounding a cell that separates the cell from its environment and consists of a phospholipid bilayer and associated proteins. Problems with the plasma membrane can be indicative of bone or heart problems, cancer, and other diseases, but only occurred in a small portion of the images. 

The mitotic spindles are responsible for evenly splitting a chromosome during mitosis into two daughter cells. Any defects in the mitotic spindles can interfere with this process and is able to indicate brain disease. Another protein that was evaluated were the aggresomes. Aggresomes protect cells by collecting and clearing misfolded proteins. In high concentrations they are able to indicate neurodegenerative disease or Epstein-Barr virus (mono). Due to the high concentration of this protein and the number of health problems they are responsible for, the team chose to focus on these for the categorization of this project. 

We wanted to determine if we could build a model that would be highly accurate for recognizing aggresomes within our images. The aggresomes model consisted of 322 samples and created by sampling down the data to images that only contained an aggresomes. These overlapped the most with nucleoplasm and cytosol. The model that was used was a simple Keras model with relu and softmax activation functions, as well as drop rate of 0.25. The dataset was highly skewed towards the aggresomes class, and there was less than 50 of most of the classes in the training data. This caused the model to overfit the training set. 

The team then decided that we wanted to downsample the skewed data to build a model for all of the classifications of proteins. The downsampled data set consisted of 6,255 samples. We achieved this by dropping all rows that did not contain rare classes. The classes were still not evenly distributed, but due to overlap in tags, they were not going to be. Multiple models were then built using this downsampled dataset and all were tested for training accuracy and testing output. 

 

For testing of the models, we trained each of our models with the down sampled dataset over 30 epochs. For each epoch we recorded the training loss, validation loss, training F score, and validation F score. 

The team knew that a good model would produce a loss value should approach 0.0 while the F-score, the harmonic mean of precision and recall, should approach 1.0 for both testing and training datasets. We watched the models carefully to see how it handled the multi-class images, and the limitations of CoLab. 

 

ModelAll had 4 features and a drop rate of 0.25, which ultimately failed as it predicted all classes for every image in the test set. ModelA was a slight improvement over the 4 feature attempt, but still needed improvement to further reduce the loss and prevent it from predicting most classes for most images. ModelC performed almost as well as ModelB but had a slightly higher loss at convergence. ModelB was the best performing model that was created and had a loss of 0.212 and F score of 0.11. 

As a group we wanted to consider what we would do differently if the experiment were to be repeated. The next time the experiment would need to possibly include more features added to the model, as well as continuously adjust cost, drop rate, and dropout rate to reduce overfitting. For this project it was a group decision to exchange accuracy for efficiency due to the limitations. We also collectively decided that it would be best to use the full data set rather that down sampling and include all four different layers of images. CoLab was not a bad tool to use for this image classification but due to its limitations, Azure would possibly be the better option.   

Website Link: https://www.kirbywhood.com/image-classification-protien-atlas
Google Drive Link: https://drive.google.com/drive/folders/1DKFKUbiQH6rI6bg8TPUsJsiawwac7ui5?usp=sharing 

E.	Big Mac Index

		Reflection
The Big Mac Index project for the Information Visualization class was an exciting one. We were able to choose any data that we wanted too and create a poster that would be intuitive to stakeholders using what we learned throughout the course. I had received an email about the top 10 data sets of 2018, and I shared it with my group. During a meeting with my team mates we had chosen to try and combine a video game data set with the Big Mac data set based on GDP per country. The original goal of this project was to see if there was a correlation between video game sales and Big Mac sales in countries around the world. We knew that this was a far-fetched idea but wanted to try something different. 

As we downloaded and pulled all of our data into R, we started to try and combine all of our variables. However, it became very apparent during one of our meetings that not all of our team mates were on the same page and knew what we were trying to accomplish. The team then refocused and decided to just focus on the Big Mac data and pull in population data with the GDP information. This again proved to be a step back from the direction that we wanted to go. There were several meetings where one or more of our team mates seemed to be disconnected or did not understand what the rest of the group was trying to accomplish. After having these discussions, we decided to just keep everything extremely simple and only use the Big Mac data set. Once that was accomplished and everyone was back on the same page, the project seemed to flow. 

We started to use ggplot and R to create visualizations so that we could recognize patterns within the data. It took several tries to get the right visualizations of the variables but working with everyone’s strengths we were able to produce wonderful visualizations. The visualizations helped us to see different countries where the Big Mac costs more or less when compared to the U.S. dollar. We had thought that some of the countries that would have had more expensive Big Macs turned out to be way below the cost of one in the U.S. This had us a bit puzzled, because they are very developed countries. After the team did some research, we found that it was due to the culture of those countries. The team chose to create all the visualizations using ggplot in R because it was easier to manage and create exactly what was needed. 

One of the biggest hurdles that the team experienced, was that we were all located in different time zones which sometime created a problem when scheduling times to meet. However, the team worked through this issue, as well as became stronger as team, when everyone was finally on the same page. This project helped me to be a better communicator with my teammates, as well as to learn more from each one of them. We all have something that we are able to bring to the table when working in a team. 

The second biggest hurdle for me during this project was presenting the poster that we had made. A previous assignment, resulted in a meeting with the professor to discuss one of my weaknesses, communication. The professor was extremely thoughtful and helped me to develop my communication skills and talk through the message that I was trying to convey. Because I was the one doing the voice over for the poster for the presentation and we had already submitted it, I asked it if would be possible to submit another voice over so that I could ensure that I had focused on the topics that needed to be discussed and discussed them clearly. The professor agreed and let me resubmit our voice over. I realized that this is still an area that I am continuing to work on and push myself to always be the communicator to the stakeholders so that I can continue to grow in this area. This project shows the mastery of the following learning objectives: collecting and organizing the data, developing alternative strategies based on differences of the data and understanding, identifying patterns using visualizations and analysis, and most importantly communication of the visualizations and analysis.

		Project Overview
Data Visualization is an important skill that a data scientist must learn to be able to help others understand what the data is saying. Of course, there are many different data visualization tools that can be used such as ggplot in R, Data Studio, Tableau, DataHero, etc. Visualizations need to be well thought out, aesthetically pleasing to the eye, with enough information to make a statement. During the Data Visualization class, it was very interesting to learn about the different tools and abilities that different software was available as well as how to create and present a well visualized story.

The Big Mac index has been published annually by The Economist since 1986 and is rated as a simplified indicator of a country’s individual purchasing power. There are around 150 products that are known worldwide, in which are monitored on a regular basis to help understand the purchasing power of countries. It was interesting to see how the Big Mac played into the purchasing power and the impact of the U.S. dollar. 

The Big Mac Index was a project that incorporated using ggplot, as well as Adobe Illustrator to create the poster, and the data was obtained from The Economist. The original data for this index analysis included 1,163 observations and 19 variables. Data was cleaned to remove missing values, and subsetted for certain plots to show “popular” country data, and the index adjusted to the U.S. dollar following the Economist’s indexing methodology. 

As a group we worked to determine if there was a relationship between a country’s status (developed verses developing) and the average price of a Big Mac related to the U.S. Price. We found that there were many countries where the price of a Big Mac was very expensive when compared to the cost in the U.S. For instance, Brazil has the most expensive Big Mac in the world, followed by Chile, Switzerland, and Thailand. Hong Kong had the least expensive Big Mac, as well as, Saudi Arabia, Japan, and Taiwan. The countries where the Big Mac costs much less than in the U.S. has to do with culture. We found that in these countries people would rather eat food originally from their home country than eat food that is imported. There were developed and underdeveloped countries on both end of the spectrum, so it was hard to determine that as a variable of influence. It was interesting to find that culture was a factor of pricing. The exchange rate also played a major roll when comparing the U.S. dollar to other currencies. This also caused some of the pricing to be lower than the U.S. dollar but could still be considered expensive within another country. 

We also worked to determine if Big Mac prices have risen or fallen in correlation with the GDP of key countries or was there no relation between price and national output. The team thought that maybe the GDP of a country would help determine the pricing of a Big Mac. The visualization that was created to explore this question determined that in some countries there is a correlation between GDP and the price of a Big Mac and others there were not. In Australia the price of a Big Mac has respectively gotten cheaper as the country has had a 0% change in GDP from 2011 to 2019. China on the other hand has had a 96% change in GDP and the price of the Big Mac has also increased about 35%. In China, we are able to see that there might be a relationship between the GDP and the price of the Big Mac. Knowing this information, further analysis might determine other correlations by gathering more economic data and making more comparisons between the price of a Big Mac and the country’s data. 

The Big Mac Index is also a useful tool for understanding currency valuations. Its recipe is the same everywhere, but maintains its popularity on six continents, across a variety of economic substrata. For example, a Big Mac in Russia costs 110 rubles ($1.65) against an adjusted U.S. price of $5.68. This suggests an undervaluation in the ruble, versus the U.S. dollar of 70%. The index also has the ability to show the strength of the U.S. dollar, which can lead to other insights when viewed with data outside the scope of this analysis. Overall, information visualization is something that is used every day to help to understand data and convey messages to the masses. 

 

Website Link: https://www.kirbywhood.com/big-mac-index
Google Drive Link: https://drive.google.com/drive/folders/1VV_jZXAOmYK1SHCX8UhevHcL46rgaRdV?usp=sharing 

III.	Importance of Data Science and Business Decisions

Many industries are able to use the techniques and applications that data science has to offer to overcome obstacles and gain insights that allow business to see different patterns within their data. These patterns are crucial in understanding financial trends, global challenges, medical implications and predictions such as flu outbreaks, supply chain, etc. Kenneth Cukier said it perfectly when it comes to the reason as to why we have so much data today. “We are collecting things that we have always collected information on, but another reason why is we are taking things that have always been informational, but have never been rendered into a data format.” 

Businesses are continuously evolving when collecting data and housing everything in data warehouses, so that it is easily accessible to analyze. Each analysis can provide major insights to consumer behavior, such as in the automotive analysis that was performed, or it can indicate different diseases within the human body. The ability to process and analyze data faster is changing the world as we know it. Fortune 500 companies are able to determine better processes for supply chain and improve productivity and profitability within distributions of goods. 

The power of these insights is incredible. Working in the marketing industry, the data that is collected on consumers is priceless, and it appears that everyone wants to know what these people are interested in, and how they view certain products. From the data that is collected we are able to indicate by search behaviors and their interactions online where that consumer might be within the buying funnel for a product. We are also able to target these particular consumers directly with advertisements, whether that be through an IP address, sociographic behaviors, or geographically. Douglas Rushkoff said, “social media is about using our data to predict our future behavior. Or when necessary to influence our future behavior so that we act in more of accordance with our statistical profiles.” Marketing through social media is becoming one of the leading ways to engage with consumers consistently, creating brand recall, and ultimately leading to sales. Data science allows businesses to use scientific methods and processes, along with algorithms to extrapolate the insights needed to make better informed business decisions. 

IV.	Conclusion

Over these months I have not only grown personally but also professionally. I have been able to go through the program and apply what I was learning each day. I have learned how to think differently and question the results against my original hypothesis to determine insights. Some of the projects that included the use of naïve Bayes, caused me to think about the posterior hypothesis and how the models changed when different variables were added. It was very exciting for me to dig into the data that was assigned as well as exploring real world scenarios to gather insights and test my knowledge. 

The above projects show the mastery of all of the learning objectives, and those that I hope to continue to hone. This program has allowed myself and my teammates to work with real clients, understand the importance of working collectively, understanding different ideas and processes, as well as the ability to take what I have learned and apply it to every day practices. It was extremely exciting for me to be able to communicate the findings of analysis and describe the visualizations and patterns that were found using different statistical methods. I also have a better understanding of the business processes and how to make recommendations to better serve a client. Correspondingly, understanding the federal, state, and local policies that are in place to protect data will help to know where the fine line rests within the sharing of digital information. 

In addition to my written portfolio, I have created a website that houses more analysis and case studies that I and teammates have accomplished. I felt that this was a wonderful way to house my work for future employers. It is arranged in a way that is easy to navigate and consists of the reported results, as well as my resume. It can be found by going to www.kirbywhood.com. I will be continuing to update the projects section and will be adding a blog with the most up to date endeavors. A colleague of mine and I have decided to continue to work together after the completion of our masters, and flush out more of our models to create an app and potentially a little start up. This program has given me the confidence to step into the field of data science and expand upon what I have learned to further my knowledge in this ever-changing field. 

